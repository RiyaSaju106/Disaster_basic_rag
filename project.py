# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qvUFbDb15cY5tQHUcKgga04BYXdUJGxD
"""

!pip install pdfplumber
!pip install gradio
!pip install pinecone-client
!pip install cohere

import pdfplumber
def extract_text_from_pdf(file_path):
    text = ""
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text()
    return text

def chunk_text(text, chunk_size=200):
    words = text.split()
    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]
    return chunks

import cohere
cohere_client = cohere.Client("tHUecSBCgRoJkpFa0LdTLkk0jTwGOemaof42Aqsc")
def get_embeddings(text):
    try:
      response = cohere_client.embed(
        model="embed-english-v2.0",
        texts=[text]
    )
      return response.embeddings[0]
    except Exception as e:
        print("Error:", e)
        return "An error occurred while retrieving embeddings."

import pinecone
from pinecone import ServerlessSpec

pinecone_client = pinecone.Pinecone(api_key="45e3e07c-dd30-447b-b26c-4e1d565ca618", environment="svc.aped-4627-b74a")

index_name = "pdfreader"
if index_name not in pinecone_client.list_indexes().names():
    pinecone_client.create_index(
        name=index_name,
        dimension=4096,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

index = pinecone_client.Index(index_name)

def store_embeddings(chunks, embeddings):
    for i, embedding in enumerate(embeddings):
        if isinstance(embedding, list) and all(isinstance(x, (int, float)) for x in embedding):
            index.upsert([(f"chunk_{i}", embedding, {"text": chunks[i]})])
        else:
            print(f"Warning: Skipping chunk {i} due to invalid embedding format: {embedding}")

import re
from collections import Counter
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer

def summarize_text(text, num_sentences=5):
    """
    Summarize a given text by extracting the most important sentences.

    Parameters:
    text (str): The input text to summarize
    num_sentences (int): Number of sentences to include in summary (default: 3)

    Returns:
    str: A summary of the input text
    """
    try:
        # Download required NLTK data
        nltk.download('punkt', quiet=True)
        nltk.download('stopwords', quiet=True)

        # Tokenize the text into sentences
        sentences = sent_tokenize(text)

        # Create word frequency table
        stop_words = set(stopwords.words('english'))
        tokenizer = RegexpTokenizer(r'\w+')
        word_freq = Counter()

        for sentence in sentences:
            # Tokenize words and convert to lowercase
            words = tokenizer.tokenize(sentence.lower())
            # Remove stopwords and update frequency
            words = [word for word in words if word not in stop_words]
            word_freq.update(words)

        # Calculate sentence scores based on word frequency
        sentence_scores = {}
        for sentence in sentences:
            words = tokenizer.tokenize(sentence.lower())
            score = sum(word_freq[word] for word in words if word not in stop_words)
            sentence_scores[sentence] = score

        # Get top N sentences with highest scores
        summary_sentences = sorted(sentence_scores.items(),
                                 key=lambda x: x[1],
                                 reverse=True)[:num_sentences]

        # Sort sentences by their original order
        summary_sentences.sort(key=lambda x: sentences.index(x[0]))

        # Join sentences to create summary
        summary = '\n'.join(sentence for sentence, score in summary_sentences)

        return summary

    except Exception as e:
        return f"Error generating summary: {str(e)}"

text = extract_text_from_pdf("/content/DisasterPeerio (2).pdf")
chunks = chunk_text(text)
embeddings = [get_embeddings(chunk) for chunk in chunks]
store_embeddings(chunks, embeddings)

def handle(question):
    question_embedding = get_embeddings(question)
    results = index.query(vector=question_embedding, top_k=5, include_metadata=True)
    relevant_chunks = [match['metadata']['text'] for match in results['matches']]
    context = '\n'.join(relevant_chunks)
    summarized_text = summarize_text(context)
    return(summarized_text)

import gradio as gr
gr.Interface(
    fn=handle,
    inputs=[gr.Textbox(label="Ask a Question")],
    outputs=[gr.Textbox(label="Response")],
    title="Disaster Peer"
).launch()