{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME9GaZ0Yx9W9"
      },
      "outputs": [],
      "source": [
        "!pip install -qU semantic-chunkers==0.0.3 datasets==2.19.1 PyPDF2 google-generativeai>=0.7.2 pdfplumber chromadb langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import chromadb\n",
        "import google.generativeai as genai\n",
        "from chromadb.utils import embedding_functions\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel, MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk\n",
        "from langdetect import detect\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define supported languages\n",
        "SUPPORTED_LANGUAGES = {\n",
        "    'en': 'English',\n",
        "    'es': 'Spanish',\n",
        "    'fr': 'French',\n",
        "    'de': 'German',\n",
        "    'it': 'Italian',\n",
        "    'pt': 'Portuguese',\n",
        "    'nl': 'Dutch',\n",
        "    'pl': 'Polish',\n",
        "    'ru': 'Russian',\n",
        "    'zh': 'Chinese',\n",
        "    'ja': 'Japanese',\n",
        "    'ko': 'Korean'\n",
        "}\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self):\n",
        "        self.translators = {}\n",
        "        # Initialize translation models only when needed\n",
        "        self.initialized_langs = set(['en'])\n",
        "\n",
        "    def initialize_language(self, lang_code):\n",
        "        if lang_code not in self.initialized_langs and lang_code != 'en':\n",
        "            try:\n",
        "                # Initialize translation to English\n",
        "                model_name = f'Helsinki-NLP/opus-mt-{lang_code}-en'\n",
        "                self.translators[f'{lang_code}_to_en'] = {\n",
        "                    'model': MarianMTModel.from_pretrained(model_name),\n",
        "                    'tokenizer': MarianTokenizer.from_pretrained(model_name)\n",
        "                }\n",
        "\n",
        "                # Initialize translation from English\n",
        "                reverse_model_name = f'Helsinki-NLP/opus-mt-en-{lang_code}'\n",
        "                self.translators[f'en_to_{lang_code}'] = {\n",
        "                    'model': MarianMTModel.from_pretrained(reverse_model_name),\n",
        "                    'tokenizer': MarianTokenizer.from_pretrained(reverse_model_name)\n",
        "                }\n",
        "\n",
        "                self.initialized_langs.add(lang_code)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not initialize {lang_code} translation: {e}\")\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def translate(self, text: str, source_lang: str, target_lang: str) -> str:\n",
        "        if source_lang == target_lang:\n",
        "            return text\n",
        "\n",
        "        if not self.initialize_language(source_lang) or not self.initialize_language(target_lang):\n",
        "            return text\n",
        "\n",
        "        try:\n",
        "            if source_lang != 'en':\n",
        "                # Translate to English first\n",
        "                translator = self.translators[f'{source_lang}_to_en']\n",
        "                inputs = translator['tokenizer'](text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "                outputs = translator['model'].generate(**inputs)\n",
        "                text = translator['tokenizer'].decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            if target_lang != 'en':\n",
        "                # Then translate to target language\n",
        "                translator = self.translators[f'en_to_{target_lang}']\n",
        "                inputs = translator['tokenizer'](text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "                outputs = translator['model'].generate(**inputs)\n",
        "                text = translator['tokenizer'].decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            print(f\"Translation error: {e}\")\n",
        "            return text\n",
        "\n",
        "class SemanticSimilarityChunker:\n",
        "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "        # Initialize with a sentence transformer model\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "    def get_embeddings(self, sentences: List[str]) -> np.ndarray:\n",
        "        \"\"\"Convert sentences to numerical vectors (embeddings)\"\"\"\n",
        "        inputs = self.tokenizer(\n",
        "            sentences,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def chunk_by_similarity(self, text: str, num_chunks: int = 20) -> List[str]:\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        grouped_sentences = [\" \".join(sentences[i:i + 5]) for i in range(0, len(sentences), 5)]\n",
        "\n",
        "        if len(grouped_sentences) < num_chunks:\n",
        "            return grouped_sentences\n",
        "\n",
        "        embeddings = self.get_embeddings(grouped_sentences)\n",
        "        kmeans = KMeans(n_clusters=num_chunks, random_state=42)\n",
        "        clusters = kmeans.fit_predict(embeddings)\n",
        "\n",
        "        chunked_text = [[] for _ in range(num_chunks)]\n",
        "        for sentence, cluster_id in zip(grouped_sentences, clusters):\n",
        "            chunked_text[cluster_id].append(sentence)\n",
        "\n",
        "        return [\" \".join(chunk) for chunk in chunked_text if chunk]\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text_data = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        total_pages = len(pdf.pages)\n",
        "        print(f\"Processing PDF with {total_pages} pages\")\n",
        "        for i, page in enumerate(pdf.pages):\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                text_data.append(text)\n",
        "            else:\n",
        "                print(f\"Empty text on page {i+1}\")\n",
        "    return text_data\n",
        "def process_text_semantically(text: str, num_chunks: int = 20) -> List[str]:\n",
        "    chunker = SemanticSimilarityChunker()\n",
        "\n",
        "    # Add overlap between chunks to maintain context\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    if len(sentences) < 10:  # For very small documents\n",
        "        return [text]\n",
        "\n",
        "    # Use smaller chunks with overlap\n",
        "    chunk_size = 10  # sentences per chunk\n",
        "    overlap = 3     # sentences of overlap\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, len(sentences), chunk_size - overlap):\n",
        "        chunk = sentences[i:i + chunk_size]\n",
        "        if chunk:\n",
        "            chunks.append(\" \".join(chunk))\n",
        "\n",
        "    # Only apply semantic clustering if we have enough chunks\n",
        "    if len(chunks) > num_chunks:\n",
        "        chunks = chunker.chunk_by_similarity(\" \".join(chunks), num_chunks)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def process_pdf_directory(directory_path):\n",
        "    all_chunks = []\n",
        "    pdf_files = [f for f in os.listdir(directory_path) if f.lower().endswith('.pdf')]\n",
        "\n",
        "    for filename in pdf_files:\n",
        "        file_path = os.path.join(directory_path, filename)\n",
        "        text_data = extract_text_from_pdf(file_path)\n",
        "        if text_data:\n",
        "            document_text = \" \".join(text_data)\n",
        "            num_chunks = max(5, len(document_text) // 1000)\n",
        "            document_chunks = process_text_semantically(document_text, num_chunks)\n",
        "            all_chunks.extend(document_chunks)\n",
        "    return all_chunks\n",
        "\n",
        "def get_user_language_preference():\n",
        "    print(\"\\nAvailable languages:\")\n",
        "    for code, name in SUPPORTED_LANGUAGES.items():\n",
        "        print(f\"{code}: {name}\")\n",
        "\n",
        "    while True:\n",
        "        lang_code = input(\"\\nPlease enter your preferred language code (e.g., 'en' for English): \").lower()\n",
        "        if lang_code in SUPPORTED_LANGUAGES:\n",
        "            return lang_code\n",
        "        print(f\"Invalid language code. Please choose from: {', '.join(SUPPORTED_LANGUAGES.keys())}\")\n",
        "\n",
        "def translate_query_response(text: str, source_lang: str, target_lang: str, translation_manager: TranslationManager) -> str:\n",
        "    \"\"\"Translate text between languages\"\"\"\n",
        "    return translation_manager.translate(text, source_lang, target_lang)\n",
        "\n",
        "def query_chroma(query, user_lang, translation_manager, collection):\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "    # Translate query to English for processing\n",
        "    english_query = translate_query_response(query, user_lang, 'en', translation_manager)\n",
        "\n",
        "    # Actually search the collection using query embeddings\n",
        "    query_results = collection.query(\n",
        "        query_texts=[english_query],\n",
        "        n_results=5,\n",
        "        include=['documents', 'distances', 'metadatas']\n",
        "    )\n",
        "\n",
        "    # Check if we got any results\n",
        "    if not query_results or not query_results['documents'] or not query_results['documents'][0]:\n",
        "        no_results_msg = \"No relevant information found in the documents.\"\n",
        "        translated_msg = translate_query_response(no_results_msg, 'en', user_lang, translation_manager)\n",
        "        print(\"\\nResponse:\", translated_msg)\n",
        "        return\n",
        "\n",
        "    # Get the relevant chunks and their similarity scores\n",
        "    relevant_chunks = query_results['documents'][0]\n",
        "    distances = query_results['distances'][0] if 'distances' in query_results else [1.0] * len(relevant_chunks)\n",
        "\n",
        "    # Convert distances to similarity scores (closer to 1 is better)\n",
        "    similarity_scores = [1 / (1 + d) for d in distances]\n",
        "\n",
        "    # Filter out low-relevance chunks\n",
        "    MIN_SIMILARITY_SCORE = 0.3\n",
        "    filtered_results = [\n",
        "        (chunk, score)\n",
        "        for chunk, score in zip(relevant_chunks, similarity_scores)\n",
        "        if score > MIN_SIMILARITY_SCORE\n",
        "    ]\n",
        "\n",
        "    if not filtered_results:\n",
        "        no_relevant_msg = \"Found some content but it wasn't relevant enough to your query.\"\n",
        "        translated_msg = translate_query_response(no_relevant_msg, 'en', user_lang, translation_manager)\n",
        "        print(\"\\nResponse:\", translated_msg)\n",
        "        return\n",
        "\n",
        "    # Prepare context from relevant chunks\n",
        "    context = \"\\n\\n\".join([\n",
        "        f\"Relevant text (similarity: {score:.2f}):\\n{chunk}\"\n",
        "        for chunk, score in filtered_results\n",
        "    ])\n",
        "\n",
        "    # Prepare the prompt for the model\n",
        "    prompt = f\"\"\"\n",
        "    Please answer this question based on the following excerpts from documents:\n",
        "\n",
        "    Question: {english_query}\n",
        "\n",
        "    Document excerpts:\n",
        "    {context}\n",
        "\n",
        "    Instructions:\n",
        "    - Base your answer only on the provided excerpts\n",
        "    - If the excerpts don't contain enough information, say so\n",
        "    - Include relevant details and cite specific information from the excerpts\n",
        "    - Be clear and concise\n",
        "    - dont give answers more than 1000 characters\n",
        "    - Answer based on the provided context\n",
        "    - If the context is partially relevant, provide a partial answer based on available information\n",
        "    - Focus on information with higher relevance scores\n",
        "    - Cite specific details from the context\n",
        "    - If you're unsure about any details, acknowledge the uncertainty\n",
        "    - Keep the response clear and factual\n",
        "\n",
        "    Answer in {SUPPORTED_LANGUAGES[user_lang]}:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Generate response using the model\n",
        "        response = model.generate_content(prompt)\n",
        "\n",
        "        if hasattr(response, 'text') and response.text:\n",
        "            # Translate response to user's language if needed\n",
        "            translated_response = translate_query_response(response.text, 'en', user_lang, translation_manager)\n",
        "            print(\"\\nResponse:\", translated_response)\n",
        "        else:\n",
        "            error_msg = \"Sorry, I couldn't generate a response. Please try rephrasing your question.\"\n",
        "            translated_error = translate_query_response(error_msg, 'en', user_lang, translation_manager)\n",
        "            print(\"\\nResponse:\", translated_error)\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"An error occurred while processing your query: {str(e)}\"\n",
        "        translated_error = translate_query_response(error_msg, 'en', user_lang, translation_manager)\n",
        "        print(\"\\nError:\", translated_error)\n",
        "\n",
        "def main():\n",
        "    # Configure Google API\n",
        "    genai.configure(api_key=\"AIzaSyDurRkk4SQwqAkZW4CEtRGkWhTioZErIWk\")  # Replace with your actual API key\n",
        "\n",
        "    # Directory containing PDF files\n",
        "    directory_path = \"Datasets\"\n",
        "\n",
        "    # Process PDFs with debug info\n",
        "    print(\"Processing PDF documents...\")\n",
        "    datasets_list = process_pdf_directory(directory_path)\n",
        "\n",
        "    if not datasets_list:\n",
        "        print(\"Error: No text was extracted from PDFs. Please check your PDF files.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Successfully extracted text from {len(datasets_list)} document sections\")\n",
        "\n",
        "    # Join all text and process semantically\n",
        "    complete_text = \" \".join(datasets_list)\n",
        "    print(f\"Total text length: {len(complete_text)} characters\")\n",
        "\n",
        "    # Process text into chunks\n",
        "    print(\"Processing text semantically...\")\n",
        "    text_chunks = process_text_semantically(complete_text)\n",
        "    print(f\"Created {len(text_chunks)} text chunks\")\n",
        "\n",
        "    # Initialize embedding model\n",
        "    print(\"Initializing embedding model...\")\n",
        "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    # Create embeddings with progress indicator\n",
        "    print(\"Creating embeddings...\")\n",
        "    embeddings = []\n",
        "    for i, chunk in enumerate(text_chunks):\n",
        "        embedding = model.encode(chunk)\n",
        "        embeddings.append(embedding)\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Processed {i + 1}/{len(text_chunks)} chunks\")\n",
        "\n",
        "    # Initialize Chroma\n",
        "    print(\"Initializing Chroma database...\")\n",
        "    client = chromadb.Client()\n",
        "    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "        \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    )\n",
        "\n",
        "    # Verify data integrity\n",
        "    if len(text_chunks) != len(embeddings):\n",
        "        raise ValueError(f\"Mismatch in lengths: chunks={len(text_chunks)}, embeddings={len(embeddings)}\")\n",
        "\n",
        "    # Reset collection\n",
        "    try:\n",
        "        client.delete_collection(name=\"my_collection\")\n",
        "    except Exception as e:\n",
        "        print(f\"Note: Could not delete existing collection: {e}\")\n",
        "\n",
        "    # Create and populate collection\n",
        "    collection = client.create_collection(\n",
        "        name=\"my_collection\",\n",
        "        embedding_function=embedding_function\n",
        "    )\n",
        "\n",
        "    # Add documents to collection with metadata\n",
        "    collection.add(\n",
        "        documents=text_chunks,\n",
        "        ids=[str(i) for i in range(len(text_chunks))],\n",
        "        embeddings=embeddings,\n",
        "        metadatas=[{\"chunk_id\": i} for i in range(len(text_chunks))]\n",
        "    )\n",
        "\n",
        "    print(f\"Successfully added {len(text_chunks)} chunks to the database\")\n",
        "\n",
        "    # Initialize translation manager\n",
        "    translation_manager = TranslationManager()\n",
        "\n",
        "    # Get user's preferred language\n",
        "    user_lang = get_user_language_preference()\n",
        "\n",
        "    # Translate prompt\n",
        "    exit_msg = translate_query_response(\n",
        "        \"Enter your query (or 'exit' to quit): \",\n",
        "        'en',\n",
        "        user_lang,\n",
        "        translation_manager\n",
        "    )\n",
        "\n",
        "    # Main query loop\n",
        "    print(\"\\nReady for queries!\")\n",
        "    while True:\n",
        "        user_query = input(exit_msg)\n",
        "        if user_query.lower() == 'exit':\n",
        "            break\n",
        "        query_chroma(user_query, user_lang, translation_manager, collection)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "UQc86lE72USB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}